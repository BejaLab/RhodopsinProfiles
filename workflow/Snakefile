import yaml
import os
import os.path
from sys import maxsize
from glob import glob

rhod_lists , = glob_wildcards("refs/{list}.fasta.pdb")

ref = 'BR'

names ,= glob_wildcards("input/{name}/references.fasta")
queries ,= glob_wildcards("queries/{query}.fasta")

hh_dbs_big   = [ "bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt", "uniclust30/uniclust30_2018_08/uniclust30_2018_08" ]
hh_dbs_small = [ "pdb70/pdb70" ]
fas_dbs = [ "uniref90/uniref90.fasta", "mgnify/mgy_clusters_2018_12.fa", "pdb_seqres/pdb_seqres.txt", "uniprot/uniprot.fasta" ]

config = {}

for name in names:
    yaml_file = "input/%s/config.yaml" % name
    with open(yaml_file) as fd:
        config[name] = yaml.safe_load(fd)

positions = []
pos_file = 'metadata/positions/%s.txt' % ref

with open(pos_file) as fh:
    for line in fh:
        pos, *rest = line.split()
        positions.append(int(pos))

class bcolors:
    HEADER  = '\033[95m'
    OKBLUE  = '\033[94m'
    OKCYAN  = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL    = '\033[91m'
    ENDC    = '\033[0m'
    BOLD    = '\033[1m'
    UNDERLINE = '\033[4m'

rule default:
    run:
        print(f"{bcolors.FAIL}No task selected - choose one of 'profile' or 'domain'{bcolors.ENDC}\n")

rule profile:
    input:
        expand("output/profile/{name}.hmm", name = names),
        expand("output/profile/{name}_{ref}.a2m", name = names, ref = ref)

rule domain:
    input:
        expand("output/domains/{query}_domains.tsv", query = queries)

rule databases:
    input:
        expand("analysis/databases/{db}_{ext}.ff{type}", db = hh_dbs_big + hh_dbs_small, ext = [ "a3m", "cs219", "hhm" ], type = [ "data", "index" ] ),
        expand("analysis/databases/{db}", db = fas_dbs),
        "analysis/databases/pdb_mmcif/mmcif_files/",
        "analysis/databases/pdb_mmcif/obsolete.dat"

rule hh_uniref:
    input:
        "output/hh_uniref_similar.tsv"

rule collect_domains:
    input:
        expand("analysis/domains/{{query}}_{name}.tsv", name = names)
    output:
        "output/domains/{query}_domains.tsv"
    params:
        names = names
    script:
        "scripts/collect_tsv.py"

rule prefilter:
    input:
        fasta = "input/{name}/references.fasta",
        outliers = "input/{name}/outliers.txt"
    output:
        filtered = "analysis/profile/{name}/filtered.fasta"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit grep -vf {input.outliers} {input.fasta} -o {output}"

rule cluster:
    input:
        "analysis/profile/{name}/filtered.fasta"
    output:
        "analysis/profile/{name}/filtered.cdhit"
    conda:
        "envs/tools.yaml"
    shell:
        "cdhit -i {input} -o {output} -d 0 -c 0.6 -n 2"

rule align:
    input:
        "analysis/profile/{name}/filtered.cdhit"
    output:
        "analysis/profile/{name}/aligned.mafft"
    threads:
        20
    conda:
        "envs/tools.yaml"
    shell:
        "mafft --thread {threads} --localpair --maxiterate 1000 --reorder {input} > {output}"

rule annotate_aln:
    input:
        "analysis/profile/{name}/aligned.mafft"
    output:
        "analysis/profile/{name}/aligned.a2m"
    conda:
        "envs/tools.yaml"
    shell:
        "reformat.pl fas a3m {input} - -v 0 -M 50 | hhconsensus -i stdin -o stdout | sed 's/>.*_consensus/>{name}_consensus/' | addss.pl stdin stdout | reformat.pl a3m a2m - {output}"

rule hmm_align:
    input:
        fas = "metadata/positions/{ref}.fas",
        hmm = "output/profile/{name}.hmm"
    output:
        "output/profile/{name}_{ref}.a2m"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmalign --outformat A2M -o {output} {input.hmm} {input.fas}"

rule hh_names_big:
    input:
        data  = "databases/{db}_a3m.ffdata",
        index = "databases/{db}_a3m.ffindex"
    output:
        "analysis/names/{db}.txt"
    wildcard_constraints:
        db = '|'.join(hh_dbs_big)
    conda:
        "envs/tools.yaml"
    shell:
        "join -t $'\\t' -1 1 -2 2 -o 2.1 1.2 <((printf '\\x0'; cat {input.data}) | grep -Pab '\\x0' | sed -e 's/:\\x0#/\\t/' -e 's/ .*//' | sort -k1b,1) <(sort -k2b,2 {input.index}) > {output}"

rule hh_names_small:
    input:
        index = "databases/{db}_a3m.ffindex"
    output:
        "analysis/names/{db}.txt"
    wildcard_constraints:
        db = '|'.join(hh_dbs_small)
    conda:
        "envs/tools.yaml"
    shell:
        "awk '{{print$1,$1}}' OFS='\\t' {input.index} > {output}"

rule convert_aln:
    input:
        "analysis/profile/{name}/aligned.a2m"
    output:
        "analysis/profile/{name}/aligned.fas"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit fx2tab {input} | tail -n+4 | seqkit tab2fx | seqkit replace -sp \\\\. -r - -o {output}"

rule hhmake:
    input:
        "analysis/profile/{name}/trimmed.a2m"
    output:
        "output/profile/{name}.hhm"
    conda:
        "envs/tools.yaml"
    shell:
        "hhmake -seq 100000 -i {input} -o {output}"

rule hhblits:
    input:
        a3m = "databases/{db}_a3m.ffdata",
        hhm = "output/profile/{name}.hhm"
    output:
        hhr = "analysis/hhblits/{db}/{name}.hhr",
        a3m = "analysis/hhblits/{db}/{name}.a3m"
    params:
        d = "databases/{db}",
        maxsize = 1000000
    conda:
        "envs/tools.yaml"
    threads:
        20
    shell:
        "hhblits -cpu {threads} -Z {params.maxsize} -B {params.maxsize} -maxseq {params.maxsize} -realign_max {params.maxsize} -i {input.hhm} -d {params.d} -o {output.hhr} -oa3m {output.a3m} -all"

rule hh_extract_names:
    input:
        hhr = "analysis/hhblits/{db}/{name}.hhr",
        names = "analysis/names/{db}.txt"
    output:
        "analysis/hhblits/{db}/{name}.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "grep -h '^>' {input.hhr} | seqkit seq -ni | sort -u | csvgrep -Ht -c2 -f- {input.names} | csvformat -TK1 > {output}"

rule hh_extract_index:
    input:
        index = "databases/{db}_a3m.ffindex",
        names = "analysis/hhblits/{db}/{name}.txt"
    output:
        "analysis/hhblits/{db}/{name}_a3m.ffindex"
    conda:
        "envs/tools.yaml"
    shell:
        "cut -f1 {input.names} | csvgrep -Ht -c1 -f- {input.index} | csvformat -TK1 > {output}"

rule hh_extract_data:
    input:
        data  = "databases/{db}_a3m.ffdata",
        index = "analysis/hhblits/{db}/{name}_a3m.ffindex"
    output:
        "analysis/hhblits/{db}/{name}.fasta"
    conda:
        "envs/tools.yaml"
    shell:
        "cut -f1 {input.index} | xargs ffindex_get {input.data} {input.index} | awk '/^#/{{c=$1}}/^>/{{NF=2;$2=c}}!/^#/' | seqkit grep -rvp _consensus$ | seqkit seq -go {output}"

rule hh_extract_index_all:
    input:
        index = "databases/{db}_{ext}.ffindex",
        names = expand("analysis/hhblits/{{db}}/{name}.txt", name = names)
    output:
        orig  = "analysis/orig_indexes/{db}_{ext}.ffindex",
        final = "analysis/databases/{db}_{ext}.ffindex"
    conda:
        "envs/tools.yaml"
    shell:
        "cut -f1 {input.names} | sort -u | csvgrep -Ht -c1 -f- {input.index} | csvformat -TK1 | tee {output.orig} | awk '{{print$1,0+s,$3;s=s+$3}}' OFS='\\t' > {output.final}"

rule hh_extract_data_all:
    input:
        data = "databases/{db}_{ext}.ffdata",
        index = "analysis/orig_indexes/{db}_{ext}.ffindex"
    output:
        "analysis/databases/{db}_{ext}.ffdata"
    conda:
        "envs/tools.yaml"
    shell:
        "ffindex_apply -q -d {output} -i /dev/null {input.data} {input.index} -- cat"

rule database_hmmsearch:
    input:
        fas = "databases/{db}",
        hmm = "output/profile/{name}.hmm"
    output:
        "analysis/hmmsearch/{db}/{name}.txt"
    conda:
        "envs/tools.yaml"
    threads:
        3
    shell:
        "hmmsearch --cpu {threads} -E 1 -o /dev/null --tblout {output} {input.hmm} {input.fas}"

rule seqkit_faidx:
    input:
        "{prefix}"
    output:
        "{prefix}.seqkit.fai"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit faidx -f {input}"

rule database_hmmsearch_extract:
    input:
        fas = "databases/{db}",
        txt = expand("analysis/hmmsearch/{{db}}/{name}.txt", name = names)
    output:
        "analysis/databases/{db}"
    wildcard_constraints:
        db = '|'.join(fas_dbs)
    conda:
        "envs/tools.yaml"
    shell:
        "grep -hv '^#' {input.txt} | cut -f1 -d' ' | sort -u | seqkit grep -f- {input.fas} > {output}"

rule select_pdb_mmcif:
    input:
        fas = "analysis/databases/pdb_seqres/pdb_seqres.txt"
    output:
        directory("analysis/databases/pdb_mmcif/mmcif_files/")
    conda:
        "envs/tools.yaml"
    threads:
        2
    shell:
        "seqkit seq -ni {input.fas} | cut -f1 -d_ | uniq | parallel -j{threads} curl --create-dirs -O --output-dir {output} -L https://files.rcsb.org/download/{{}}.cif"

rule filter_obsolete:
    input:
        fas = "analysis/databases/pdb_seqres/pdb_seqres.txt",
        dat = "databases/pdb_mmcif/obsolete.dat"
    output:
        "analysis/databases/pdb_mmcif/obsolete.dat"
    conda:
        "envs/tools.yaml"
    shell:
        "(head -n1 {input.dat}; seqkit seq -ni {input.fas} | cut -f1 -d_ | uniq | grep -if- {input.dat}) > {output}"

rule hh_blast:
    input:
        query = "analysis/hhblits/{name}/bfd.fasta",
        db = "databases/{list}.fasta"
    output:
        "analysis/hhblits/{name}/bfd-{list}.blast"
    params:
        evalue = 1e-10
    conda:
        "envs/tools.yaml"
    shell:
        "blastp -db {input.db} -query {input.query} -evalue {params.evalue} -out {output} -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle'"

rule hh_hhm:
    input:
        query = "analysis/hhblits/{name}/bfd.fasta",
        hmm = "output/profile/{name}.hmm"
    output:
        tblout = "analysis/hhblits/{name}/bfd.hmmsearch.tblout",
        domtblout = "analysis/hhblits/{name}/bfd.hmmsearch.domtblout",
        out = "analysis/hhblits/{name}/bfd.hmmsearch.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch --max --tblout {output.tblout} --domtblout {output.domtblout} -o {output.out} {input.hmm} {input.query}"

rule hhalign_BR:
    input:
        i = "metadata/positions/{ref}.fas",
        t = "analysis/hhblits/{name}/bfd.a3m"
    output:
        "analysis/hhblits/{name}/{ref}.hhr"
    conda:
        "envs/tools.yaml"
    shell:
        "hhalign -i {input.i} -t {input.t} -o {output}"

rule extract_hhr_positions:
    input:
        a3m = "analysis/hhblits/{name}/bfd.a3m",
        ref_hhr = "analysis/hhblits/{name}/{ref}.hhr"
    output:
        "analysis/hhblits/{name}/bfd_{ref}.tsv"
    params:
        positions = positions
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/extract_a3m.py"

rule trim_aln:
    input:
        a2m = "analysis/profile/{name}/aligned.a2m"
    output:
        trimmed = "analysis/profile/{name}/trimmed.a2m"
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/trim.py"

rule build_hmm:
    input:
        "analysis/profile/{name}/trimmed.a2m"
    output:
        "output/profile/{name}.hmm"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmbuild -n {wildcards.name} {output} {input}"

rule hmmsearch:
    input:
        fasta = "queries/{query}.fasta",
        hmm = "output/profile/{name}.hmm"
    output:
        matches = "analysis/domains/{query}_{name}.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o {output} --max {input.hmm} {input.fasta}"

#rule xlsx_to_fasta:
#    input:
#        "queries/{query}.xlsx"
#    output:
#        "queries/{query}.fasta"
#    conda:
#        "envs/r.yaml"
#    script:
#        "scripts/xlsx2fasta.R"

rule extract_domains:
    input:
        fasta = "queries/{query}.fasta",
        a2m = expand("output/profile/{{name}}_{ref}.a2m", ref = ref),
        matches = "analysis/domains/{query}_{name}.txt",
        hmm = "output/profile/{name}.hmm"
    output:
        "analysis/domains/{query}_{name}.tsv",
    params:
        config = lambda w: config[w.name],
        positions = positions
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/extract_hmmsearch.py"

rule cdhit:
    input:
        "output/domains/{query}_domains.fasta"
    output:
        cdhit = "output/domains/{query}_domains_{c}.cdhit",
        clstr = "output/domains/{query}_domains_{c}.cdhit.clstr"
    params:
        n = lambda w: 2 if float(w.c) <= 0.6 else 5
    threads:
        10
    conda:
        "envs/tools.yaml"
    shell:
        "cdhit -i {input} -o {output.cdhit} -c {wildcards.c} -d 0 -n {params.n} -T {threads}"

rule tsv_to_fasta:
    input:
        "output/domains/{query}_domains.tsv"
    output:
        "output/domains/{query}_domains.fasta"
    params:
        min_len = 180
    conda:
        "envs/tools.yaml"
    shell:
        "csvcut -t -c record_id,domain_seq,positions_res {input} | csvformat -T | awk 'NR>1&&$3!~/-/&&$3~/K$/' | cut -f1,2 | seqkit tab2fx | seqkit seq -gm{params.min_len} -o {output}"

rule cat:
    input:
        "queries/sequences.fasta",
        expand("analysis/uniref100/{profile}/hmmsearch.fasta", profile = names),
        expand("analysis/hhblits/{profile}/bfd.fasta", profile = names)
    output:
        "queries/database.fasta"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit rmdup -o {output} {input}"

# Not under conda
rule usearch_global:
    input:
        query = "output/domains/sequences_domains.fasta",
        target = "output/domains/database_domains.fasta"
    output:
        "analysis/global/usearch_global.tsv"
    params:
        id = 0.5,
        query_cov = 0.95
    threads:
        10
    shell:
        "usearch -threads {threads} -usearch_global {input.query} -db {input.target} -id {params.id} -userout {output} -userfields query+target+id -query_cov {params.query_cov} -target_cov 0 -maxaccepts 1000000"

rule usearch_extract:
    input:
        tsv = "analysis/global/usearch_global.tsv",
        fasta = "queries/database.fasta",
        fai = "queries/database.fasta.seqkit.fai"
    output:
        "analysis/global/usearch_global.fasta"
    conda:
        "envs/tools.yaml"
    shell:
        "cut -f2 {input.tsv} | cut -f1 -d' ' | sort -u | xargs seqkit faidx -f {input.fasta} > {output}"

rule uniref_hmmsearch:
    input:
        fas = "refs/uniref100.fasta",
        hmm = "output/profile/{name}.hmm"
    output:
        "analysis/uniref100/{name}/hmmsearch.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -E 1 -o /dev/null --tblout {output} {input.hmm} {input.fas}"

rule uniref_matches:
    input:
        "analysis/uniref100/{name}/hmmsearch.txt"
    output:
        "analysis/uniref100/{name}/hmmsearch.list"
    conda:
        "envs/tools.yaml"
    shell:
        "grep -v '^#' {input} | cut -f1 -d' ' > {output}"

rule uniref_extract:
    input:
        fasta = "refs/uniref100.fasta",
        list  = "analysis/uniref100/{name}/hmmsearch.list"
    output:
        "analysis/uniref100/{name}/hmmsearch.fasta"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit grep -f {input.list} -o {output} {input.fasta}"

rule hh_uniref_collect:
    input:
        usearch = "analysis/global/usearch_global.tsv",
        clstr   = "output/domains/database_domains_0.5.cdhit.clstr",
        fasta   = "queries/database.fasta",
        cdhit   = "output/domains/database_domains_1.cdhit"
    output:
        similar = "output/hh_uniref_similar.tsv",
        different = "output/hh_uniref_different.tsv"
    conda:
        "envs/r.yaml"
    script:
        "scripts/global.R"
